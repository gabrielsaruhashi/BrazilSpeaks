---
title: "BrazilSpeaks"
author: "Gabriel Saruhashi"
date: "3/17/2019"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro
On April 1, 1964, the military organized a coup d’état that overthrew the government of president João Goulart. That day marked the beginning of the Military Dictatorship that lasted for twenty-one years. Under the pretext of eliminating the growing Communist threat, the regime suppressed freedom of speech and imposed rigorous censorship over the all forms of media. In the late 60s, with the popularization of television and radio stations, music began to have a lot of influence over society and, for this reason, it was heavily monitored by the regime’s censors. On the one hand, there was a group of musicians that simply conformed to the oppressive rules of the regime. Inspired by the soft rock melodies by the Beatles, they avoided political themes and made fortunes composing songs about love and trivial, middle-class concerns. Yet, on the other hand, a group of musicians stood out in the fight against oppression. Through their music, they conveyed a message of criticism against the regime. Their “protest music” denounced blatant social injustices, mobilized political passions, praised the individual and collective heroes who fought the oppressors. In this project, I will be analyzing this dataset I built 

# DATA
## Data Scraping & Collection
To collect the data,  I followed the work done by Carocha (2006). I built a Python script that called the Spotify API and ran a data enrichment pielin 

I compiled two Spotify playlists, one for each class of music. Through the Spotify API, I obtained key features of each song, such as speechness, danceability and energy, that are measured in a scale of 0.0 to 1.0.  However, Spotify does not directly provide the lyrics for each of the songs. To circumvent this limitation, I built a parallel pipeline that, given a song name ands its author, scrapes song lyrics from Genius and Vagalume, two well-known music platform that provide lyrics and song annotations. The procedure yielded a corpus of 280 songs equally divided in the two categories: 140 censored and 140 uncensored songs. 

```{python python.reticulate=FALSE, eval=FALSE, error=FALSE, echo=FALSE}
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from spotipy.oauth2 import SpotifyClientCredentials
import spotipy
import json
import requests
from bs4 import BeautifulSoup
import pandas as pd
import pprint
import time
from nltk.stem import RSLPStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import os
import re 

def setEnvironmentVariables():
    os.environ['SPOTIPY_CLIENT_ID'] = 'c894a126681b4d97a8ccb0cd4a1e0de1'
    os.environ['SPOTIPY_CLIENT_SECRET'] = 'ebf185aaf47e40ab841246986fc7483d'
    os.environ['SPOTIPY_REDIRECT_URI'] = 'https://localhost:8080'
    print('Successfully set the environment variables')

def requestSongInfo(song_title, artist_name):
    base_url = 'https://api.genius.com'
    headers = {'Authorization': 'Bearer ' + '0RIKjAuJB6gohq-1r-w7FzG7W3FcgsL2ZwSRWjUdLLH0E31lUt6T8otW-JDL7VYC'}
    search_url = base_url + '/search'
    data = {'q': song_title + ' ' + artist_name}
    response = requests.get(search_url, data=data, headers=headers)

    return response

def scrapeSongURL(url):
    print("scraping {}".format(url))
    page = requests.get(url)
    html = BeautifulSoup(page.text, 'html.parser')
    lyrics = html.find('div', class_='lyrics').get_text()

    return lyrics

# Preprocessing of the Lyrics
def preprocessLyrics(sentence):
    # stemmer=RSLPStemmer()

    sentence = sentence.lower()
    # remove all the annotations (e.g '[refrão 1] Bla bla')
    sentence = re.sub(r'[\(\[].*?[\)\]]', "", str(sentence))

    # get Portuguese stopwords
    file_stop = open("pt_stopwords.txt")
    body_stop = file_stop.read()
    stop = body_stop.split()

    token_words = word_tokenize(sentence)
    processed_sentence=[]
    
    for word in token_words:
        if word not in stop:
            processed_sentence.append(word)
            # stem_sentence.append(stemmer.stem(word))
            processed_sentence.append(" ")
    
    # remove all the annotations within [] and ()
    
    return "".join(processed_sentence)

def extractLyrics(song_title, artist_name):
    # Search for matches in request response
    response = requestSongInfo(song_title, artist_name)
    json = response.json()
    remote_song_info = None
    

    for hit in json['response']['hits']:
        if artist_name.lower() in hit['result']['primary_artist']['name'].lower():
            remote_song_info = hit
            break

    # Extract lyrics from URL if song was found
    if remote_song_info:
        song_url = remote_song_info['result']['url']
        lyrics = scrapeSongURL(song_url)
        lyrics = lyrics.replace('\n', ' ')
        lyrics = preprocessLyrics(lyrics)
            
        return lyrics
    else:
        print("Could not find lyrics for given artist and song title")
        return ""

def getSpotifySongFeatures(uri):
    song_features = sp.audio_features(uri)
    song_features = song_features[0]
    
    extra_fields = ["track_href", "uri", "analysis_url", "type"] 

    for field in extra_fields:
        song_features.pop(field)

    return song_features

def getSpotifyArtistInfo(artist_id):
    artist = {}
    
    info = sp.artist(artist_id)
   
    artist["artist_genres"] = info["genres"][0]
    artist["artist_name"] = info["name"]
    if info["images"]:
        artist["artist_photo"] = info["images"][0]["url"]
    else:
        artist["artist_photo"] = ""
    artist["artist_popularity"] = info["popularity"]
    artist["artist_sp_followers"] = info["followers"]["total"]

    return artist

def processSpotifyPlaylistCSV(uri, csv_filepath, song_class):
    start_time = time.time()

    username = uri.split(':')[2]
    playlist_id = uri.split(':')[4]

    # get the relevant playlist
    results = sp.user_playlist(username, playlist_id)

    tracks = results["tracks"]["items"]

    # define main data frame that will store 
    df = pd.DataFrame()
    index = 0
    for obj in tracks:    
        track = obj["track"]
        song = {}
        
        # preprocessed song name
        song_name = re.split(r' -| \(', track["name"])[0]

        # song["artist"] = artist
        song["song_sp_uri"] = track["uri"]
        song["song_name"] = song_name
        song["song_isrc"] = track["external_ids"]["isrc"]
        song["song_popularity"] = track["popularity"]
        song_features = getSpotifySongFeatures(track["uri"])

        artist_info = getSpotifyArtistInfo(track["artists"][0]["id"])
        song["song_lyrics"] = extractLyrics(song["song_name"], artist_info["artist_name"])
        song["class"] = song_class

        # concatenating all dictionaries
        song = {**song, **song_features, **artist_info}   
   
        df = pd.concat([df, pd.DataFrame(song, index=[index])])
        index += 1

    print("Scraping process took {} s. Now storing intermediate results for this class of music".format(time.time() - start_time))
    df.to_csv(csv_filepath)

    return df
    
# Uncomment this section if you'd like to start the datascraping script
'''
PROTEST_URI = 'spotify:user:gabriel_saruhashi:playlist:4Tp4QcTk9rNikjmaDg5VxJ'
JOVEM_GUARDA_URI = 'spotify:user:gabriel_saruhashi:playlist:1JZoMCGiAKcXrgBzbKW931'
PROTEST_CLASSNAME = "Protest"
JOVEM_GUARDA_CLASSNAME = "Jovem Guarda"
setEnvironmentVariables()

client_credentials_manager = SpotifyClientCredentials()
sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

# create csv with data from spotify
protest_df = processSpotifyPlaylistCSV(PROTEST_URI, "protest.csv", "Protest")
jovem_guarda_df = processSpotifyPlaylistCSV(JOVEM_GUARDA_URI, "jovem_guarda.csv", "Jovem Guarda")

# store final output
res_df = pd.concat([protest_df, jovem_guarda_df])
res_df.to_csv("brz_dictatorship.csv")
'''
```

## Overview of the data
Upon loading the data, we observe the following structure:

* song_sp_uri (chr): a unique identifies  song in the spotify platform
* song_name (chr): the name of the song
* song_isrc (chr): the International Standard Recording Code for the song
* song_popularity (int) : provided by the Spotify API, "The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are"
* song_lyrics (chr): the lyrics of the song scraped from Genius and Vagalume
* class (chr): the class of the song (either protest or Young Guard) according to the definition presented in the intro
* danceability (num): provided by the Spotify API, "a value of 0.0 is least danceable and 1.0 is most danceable."
* energy (num): provided by the Spotify API, "energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity"
* key (int): provided by the Spotify API, "the estimated overall key of the track"
* loudness (int): provided by the Spotify API, "the overall loudness of a track in decibels (dB)"
* mode 
* speechiness (num): provided by the Spotify API, "float	Speechiness detects the presence of spoken words in a track"
* acousticness (num): provided by the Spotify API, "A confidence measure from 0.0 to 1.0 of whether the track is acoustic"
* instrumentalness (num): provided by the Spotify API, "predicts whether a track contains no vocals"
* liveness (num): provided by the Spotify API, "detects the presence of an audience in the recording. . Higher liveness values represent an increased probability that the track was performed live"
* valence (num): provided by the Spotify API, "a measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track"
* tempo (num): provided by the Spotify API, "the overall estimated tempo of a track in beats per minute"
* id (chr): the Spotify ID for the artist
* duration_ms: provided by the Spotify API, "the duration of the track in milliseconds"
* time_signature (int)
* artist_genres (chr): provided by the Spotify API, "a list of the genres the artist is associated with
* artist_name (chr): the name of the artist
* artist_photo: (chr): url to the photo of the artist
* artist_popularity (int): provided by the Spotify API, "the value will be between 0 and 100, with 100 being the most popular. "
* artist_sp_followers (int):  542214 542214 542214 299597 829961 532021 542214 16490 2440436 299597
```{r echo=FALSE}
music = read.csv("brz_dictatorship.csv", as.is=TRUE)
print("Number of dimensions in our dataset (row, col):")
dim(music)
```
```{r}
unique(music$artist_name[music$class=='Protest'])

jg_artists = unique(music$artist_name[music$class=='Jovem Guarda'])

capture.output(jg_artists, file = "data/artists_jg.txt")


```

Create corpus for text mining
```{r}
library(tm)

print("Creating corpus by collapsing together both protest music and Young Guard music")
jg <- paste(music$song_lyrics[music$class=="Jovem Guarda"], collapse = '')
protest <- paste(music$song_lyrics[music$class=="Protest"], collapse = '')
docs <- Corpus(VectorSource(c(jg, protest)))
str(docs)

```

# Data Cleaning
Although the song features supplied by the Spotify API were already normalized, I had to perform some preprocessing of the lyric. First, I removed stopwords (e.g ‘me’, ‘I’, etc.) from the dataset given that they are so common in the language that their informational value is near zero. Second I cleaned up the Genius lyrics by removing the annotations, punctuations and number. 

```{r warning=FALSE}
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")

# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove Portuguese common stopwords
docs <- tm_map(docs, removeWords, stopwords("portuguese"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("mim", "pra", "vai")) 
```

# Descriptive Plots & Summary Information

## Lyric Analysis
Inspired by the analysis conducted by (http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know)
First, I plotted a word cloud with the most frequent words for each class of music. Uncensored music had much more positive lyrics, with words such as love, romance and joy standing out, whereas protest music had more descriptive words such as violence, blood, etc. 
```{r warning=FALSE}

# Document matrix is a table containing the frequency of the words. Column names are words and row names are documents
dtm_jg <- TermDocumentMatrix(docs[1])
m <- as.matrix(dtm_jg)
v <- sort(rowSums(m),decreasing=TRUE)
d_jg <- data.frame(word = names(v),freq=v)
head(d_jg, 10)

dtm_protest <- TermDocumentMatrix(docs[2])
m <- as.matrix(dtm_protest)
v <- sort(rowSums(m),decreasing=TRUE)
d_protest <- data.frame(word = names(v),freq=v)
head(d_protest, 10)

set.seed(1234)
wordcloud(words = d_protest$word, freq = d_protest$freq, min.freq = 15,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
wordcloud(words = d_jg$word, freq = d_jg$freq, min.freq = 15,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Then I performed ANOVA across four main song features, namely speechiness, energy, danceability and valence. As I imagined, protest music had higher speechiness given that the protest musicians prioritized the content of the message over form or harmonic features, whereas uncensored music had higher valence, danceability and energy. These characteristics were also in line with the insights gained from the historical study given that the Young Guard were known for their sappy songs that were popular in parties and bars (Table 1). All p-values were significant (p < 0.05).
 
```{r echo=FALSE}
library("ggplot2")
# 1. Open jpeg file
boxplot(valence ~ class, data=music, col = 'yellow', main = "Valence by Class", ylab = "Valence")
#calculate means using the tapply function - could also use the by function
means <- tapply(music$valence, music$class, mean)
points(means, col = "red", pch = 19, cex = 1.2)
text(x=c(1:5), y=means+.2, labels = round(means,2))

boxplot(speechiness ~ class, data=music, col = 'green', main = "Speechiness by Class", ylab = "Speechiness")
#calculate means using the tapply function - could also use the by function
means <- tapply(music$speechiness, music$class, mean)
points(means, col = "red", pch = 19, cex = 1.2)
text(x=c(1:5), y=means+.2, labels = round(means,2))

boxplot(energy ~ class, data=music, col = 'blue', main = "Energy by Class", ylab = "Energy")
#calculate means using the tapply function - could also use the by function
means <- tapply(music$energy, music$class, mean)
points(means, col = "red", pch = 19, cex = 1.2)
text(x=c(1:5), y=means+.2, labels = round(means,2))

boxplot(danceability ~ class, data=music, col = 'pink', 
        main = "Danceability by Class", ylab = "Danceability")
#calculate means using the tapply function - could also use the by function
means <- tapply(music$danceability, music$class, mean)
points(means, col = "red", pch = 19, cex = 1.2)
text(x=c(1:5), y=means+.2, labels = round(means,2))



boxplot(log(speechiness) ~ class, data=music, col = 'green', main = "Speechiness by Class", ylab = "Speechiness")
#calculate means using the tapply function - could also use the by function
means <- tapply(log(music$speechiness), music$class, mean)
points(means, col = "red", pch = 19, cex = 1.2)
text(x=c(1:5), y=means+.2, labels = round(means,2))

```
From the boxplots above, it seems that there is visual evidence for a significant differences between the two classes of music (Jovem Guarda and Protest). Let's conduct some t-tests to evaluate if these differences are significant.
```{r echo=FALSE}
(test1 <- t.test(valence ~ class, data=music)$conf.int)
(test2 <- t.test(speechiness ~ class, data=music)$conf.int)
(test3 <- t.test(energy ~ class, data=music)$conf.int)
(test4 <- t.test(danceability ~ class, data=music)$conf.int)

```
### Permutation Test
Given that our t-tests p-values are significant, let's conduct a bootstrap test on Valence to look for the confidence intervals for the means difference in valence between the two classes.
```{r }
N <- 10000
diffValence <- rep(NA, N)
set.seed(1)    #This is so we get same results every time

for (i in 1:N) {
  sA <- sample(music$valence[music$class == "Protest"],
               sum(music$class == "Protest"), replace = TRUE)
  sB <- sample(music$valence[music$class == "Jovem Guarda"],
               sum(music$class == "Jovem Guarda"), replace = TRUE)
  diffValence[i] <-  mean(sB) - mean(sA)
}

boot_ci <- quantile(diffValence, c(0.025, 0.975))

#Make histogram of bootstrap sample means
hist(diffValence, col = "blue", main = "Bootstrapped Sample Means Diff in Valence", xlab = "Valence", breaks = 50)

#Add lines to histogram for CI's
abline(v=boot_ci,lwd=3, col="red")
abline(v=test1,lwd=3, col="green", lty = 2)
legend(48,600, c("Original CI","Boot CI"), lwd=3, col = c("green","red"), lty = c(2,1))
```


```{r echo=FALSE}
#Function to get permutation test p-values for correlation

options(scipen=999)
permCor <- function(x, y, n_samp = 10000, plot = T){
   corResults <- rep(NA, n_samp)
   for (i in 1:n_samp){
      corResults[i] <- cor(x, sample(y))
   }
   pval <- mean(abs(corResults) >= abs(cor(x,y)))
   if (plot == T){
      #Make histogram of permuted correlations
      hist(corResults, col = "yellow", main = "", xlab = "Correlations", breaks = 50,
           xlim = range(corResults,cor(x,y)))
      mtext("Permuted Sample Correlations", cex = 1.2, line = 1)
      mtext(paste("Permuted P-value =",round(pval,5)), cex = 1, line = 0)
      abline(v = cor(x,y), col="blue", lwd=3)
      text(cor(x,y)*1.05, 0,paste("Actual Correlation =", round(cor(x,y),2)),srt = 90, adj = 0)
   }
   if (plot == F){
      return(round(pval,5))
   }  
}
#cor(music$valence,music$danceability)
permCor(music$valence,music$danceability)
```
The null hypothesis is that there is no difference in the median of improvements between male and female runners. The alternative hypothesis is that there is a difference in the median of improvements between male and female runners. We cannot reject the null hypothesis in this case given that the difference is not statistically significant (0.08 > 0.05). In the context of this test, this p-value is the probability of finding a test statistic (i.e difference in mean) for the group comparison at least as high as the one observed, provided that there is no actual difference (i.e., null hypothesis is true).
## Basic tests with the different classes

```{r echo=FALSE}

#Load the corrplot package
library(corrplot)

df <- music[,8:18]
cor1 <- cor(df, use="pairwise.complete.obs")

#get the array index (row, col) for the predictors of maximum non-one correlation value
#By ignoring correlations cor1 == 1, you discard the matrix main diagonal
#(correlation of a variable with itself is always 1).
maxloc <- which(cor1 == max(cor1[cor1<1]), arr.ind = TRUE)

#get the column names of the two variables with highest correlation by index
#note that maxloc[2, ] is the same as maxloc[1, ], but flipped
print("The two column names of the two variables with the highest correlation:")
names(df)[maxloc[1,]]

#Create an object called sigcorr that has the results of cor.mtest for columns 10-23 of the crime data.  Use 95% CI.
sigcorr <- cor.mtest(df, conf.level = .95)

#Use corrplot.mixed to display confidence ellipses, pairwise correlation values, and put on 'X' over non-significant values.
corrplot.mixed(cor1, 
               lower.col="black", 
               upper = "ellipse",
               tl.col = "black", 
               number.cex=.7, 
               tl.pos = "lt", 
               tl.cex=.7, 
               p.mat = sigcorr$p, 
               sig.level = .05)

```

Now let's examine more closely the correlation between the two variables with highest correlation.

```{r echo=FALSE}
plot(jitter(loudness) ~ jitter(energy), 
     data=df,
     xlab="Song Energy",
     ylab="Song Loudness",
     main=paste("Jittered scatterplot for loudness and energy\nSample correlation", round(cor1[maxloc[1, 1], maxloc[1,2]], 2)),
     col="blue")
```
By adding a small amount of random normally distributed noise, we can see observations and their densities more clearly, and now it looks like there is a strong correlation between the two song features (as demonstrated by the slighlty linear concentration in density).


## Stepwise Regression
We are now going to proceed with performing stepwise regression. In particular, we're going to fit a model that looks at possible predictors of the class of the song.   To do this, I'm making a new dataset called `music2` which contains the relevant columns (notice I'm putting the response variable FIRST).  Be sure to remove the option `eval = F`.

```{r}
#avoid multicolinaeartiy issues
music2 <- music[,c(5, 8:18)]

names(music2)
dim(music2)

total_vars <- dim(music2)[2]

```
Perform best subsets regression using the `regsubsets` function in the `leaps` package.  Save the results in an object called `mod2`.  Get the summary of `mod2` and save the results in an object called `mod2sum`.  Display `mod2sum$which` to get a sense of which variables are included at each step of best subsets.

```{r echo = T, results = 'hide'}
library('leaps')
mod2 <- regsubsets(song_popularity ~ ., data=music2, nvmax=total_vars)
mod2sum <- summary(mod2)
```

Now, let's examine the best model according to highest r-squared, etc.
```{r}
(modnum = which.max(mod2sum$rsq))

#Which variables are in model 12
names(music2)[mod2sum$which[modnum,]][-1]

#Fit this model and show results
musictemp <- music2[,mod2sum$which[modnum,]]

#summary(lm(song_popularity ~ .,data=musictemp))

```

```{r}
(modnum <- which.max(mod2sum$adjr2))

#Which variables are in model 12
names(music2)[mod2sum$which[modnum,]][-1]

#Fit this model and show results
musictemp <- music2[,mod2sum$which[modnum,]]
#summary(lm(song_popularity ~ .,data=musictemp))
```

BIC
```{r }
(modnum = which.min(mod2sum$bic))

#Which variables are in model 12
names(music2)[mod2sum$which[modnum,]][-1]

#Fit this model and show results
musictemp <- music2[,mod2sum$which[modnum,]]

#summary(lm(song_popularity ~ .,data=musictemp))
```
CP
```{r}
(modCP <- min(c(1:length(mod2sum$cp))[mod2sum$cp < c(1:length(mod2sum$cp))+1]))

#Which variables are in model 2
names(music2)[mod2sum$which[modCP,]][-1]

#Fit this model and show results
musictemp <- music2[,mod2sum$which[modCP,]]
#summary(lm(song_popularity ~ .,data=musictemp))
```

Now, let's evaluate the final model we have:
```{r}
musicfinal <- music2[,mod2sum$which[1,]]
modfin <- lm(song_popularity ~ .,data=musicfinal)

#get new function for pairs plotn AND get myResPlots function
source("http://www.reuningscherer.net/s&ds230/Rfuncs/regJDRS.txt")

myResPlots2(modfin,"Model for Song Popularity")
```
The model assumptions do not seem be met given that the erros in the normal quantile plot are NOT normally distributed and the fits vs residuals plot show no evidence of heteroscedasticity (the distribution of positive and negative residuals appears symmetric across all the possible fitted values). in other words, the assumptions are met because there is constant variance across fitted values, few outliers, no clear trend.


